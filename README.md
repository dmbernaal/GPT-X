This repo contains my notes from Andrej Kaparthy's **Lets build GPT: from scratch, in code, spelled out.** video which can be found here: [video lecture]("https://www.youtube.com/watch?v=kCc8FmEb1nY&t=5664s").

This repo will also be my attempt to build onto what Andrej has built with nanoGPT. The ultimate goal will be build an entire infrastructure to train GPT variants with PyTorch using much of what I have learned from **fastai**.

The idea is to build a training infrastructure which will allow me to experiment with GPT variants such as:

- Model architecture / topology
- Optimization methods (Adam seems to be standard for GPT, I will post results with various other optimizers)
- Scheduling techniques (1-cycle training, etc).

Overall, this is a fun project for myself and would love for others to gain something from this!
