{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloading the shakespear dataset to quickly build and experiment\n",
    "# NOTE: For a large dataset which is similar to what openai has used use this: https://huggingface.co/datasets/openwebtext\n",
    "URL = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "response = requests.get(URL).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n"
     ]
    }
   ],
   "source": [
    "print(response[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters:  65\n"
     ]
    }
   ],
   "source": [
    "# since we are building a character level model, which is a model that is able to predict the next character and not the next word, we will need to build a character level vocabulary\n",
    "# TODO: In the future, we can implement tokenizers via HuggingFace's tokenizers library\n",
    "chars = sorted(list(set(response)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(\"Total number of characters: \", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's build a basic tokenizer, this tokenizer is a character level tokenizer\n",
    "# NOTE: a more useful tokenization library may be using something like tiktoken which is openais tokenizer for their gpt-x models\n",
    "# link here: https://github.com/openai/tiktoken TODO: implement this tokenizer\n",
    "class Tokenizer:\n",
    "    def __init__(self, chars, vocab_size):\n",
    "        self.chars = chars\n",
    "        self.vocab_size = vocab_size\n",
    "        self.s2i = {char: idx for idx, char in enumerate(chars)} # character to index\n",
    "        self.i2s = {idx: char for idx, char in enumerate(chars)} # index to character\n",
    "\n",
    "    def encode(self, text, return_tensor=False):\n",
    "        \"\"\"This function encodes the text into tokens\"\"\"\n",
    "        encoded_list = [self.s2i[char] for char in text]\n",
    "        if return_tensor: return torch.tensor(encoded_list, dtype=torch.long)\n",
    "        return encoded_list\n",
    "    \n",
    "    def decode(self, tokens):\n",
    "        \"\"\"This function decodes the tokens into text\"\"\"\n",
    "        # if tokens are a pytorch tensor, convert it to a list\n",
    "        if type(tokens) == torch.Tensor:tokens = tokens.tolist()\n",
    "        return \"\".join([self.i2s[token] for token in tokens])\n",
    "    \n",
    "    @classmethod\n",
    "    def from_text(cls, text):\n",
    "        \"\"\"This function creates a tokenizer from the text\"\"\"\n",
    "        chars = sorted(list(set(text)))\n",
    "        vocab_size = len(chars)\n",
    "        return cls(chars, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters:  65\n"
     ]
    }
   ],
   "source": [
    "# let's instantiate our tokenizer\n",
    "tokenizer = Tokenizer.from_text(response) # from our text\n",
    "\n",
    "# as we can see if contains the same character length as before\n",
    "print(\"Total number of characters: \", tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[46, 47, 47, 1, 58, 46, 43, 56, 43]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's now encode sample text\n",
    "tokenizer.encode(\"hii there\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hii there'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and we can decode it back\n",
    "tokenizer.decode(tokenizer.encode(\"hii there\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets now build a dataset from our text data which will use our tokenizer to encode the text\n",
    "class CharDataset:\n",
    "    def __init__(self, text, tokenizer, seq_len=128):\n",
    "        self.text = text\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_len = seq_len\n",
    "        self.text_encoded = self.tokenizer.encode(self.text, return_tensor=True) # all our text encoded\n",
    "        self.total_len = len(self.text_encoded)\n",
    "\n",
    "    @classmethod\n",
    "    def from_text(cls, text, tokenizer_cls=None, seq_len=128):\n",
    "        tokenizer = Tokenizer.from_text(text) if tokenizer_cls is None else tokenizer_cls.from_text(text)\n",
    "        return cls(text, tokenizer, seq_len)\n",
    "    \n",
    "    def __len__(self): return self.total_len // self.seq_len\n",
    "    def __getitem__(self, idx): return self.text_encoded[idx:idx+self.seq_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of our dataset is:  4357\n",
      "The vocab size is :  65\n"
     ]
    }
   ],
   "source": [
    "# lets now instantiate our dataset\n",
    "dataset = CharDataset.from_text(response, seq_len=128*2)\n",
    "\n",
    "print(\"The length of our dataset is: \", len(dataset))\n",
    "print(\"The vocab size is : \", dataset.tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0])\n"
     ]
    }
   ],
   "source": [
    "# showing an example of the dataset\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we can decode this too\n",
    "print(dataset.tokenizer.decode(dataset[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0638b84c441d23f3bf1e5bbb68dbbbae5f508c99744b50e7a508082753ac4090"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
